# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LnpIgxjxKOSRFy2tXenZi7YrG9RT_odH
"""

pip install flask pdfplumber sentence-transformers faiss-cpu langchain openai tiktoken

# Commented out IPython magic to ensure Python compatibility.
# %env OPENAI_API_KEY=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

import pdfplumber
from sentence_transformers import SentenceTransformer

file_path='/content/Tables- Charts- and Graphs with Examples from History- Economics- Education- Psychology- Urban Affairs and Everyday Life - 2017-2018.pdf'
def extract_and_chunk_pdf(file_path, chunk_size=200):
    """Extracts and chunks text from a PDF."""
    with pdfplumber.open(file_path) as pdf:
        text = ''
        for page in pdf.pages:
            text += page.extract_text()

    # Chunking the text into smaller granularity
    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]
    return chunks

def create_embeddings(chunks):
    """Creates embeddings for text chunks."""
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode(chunks)
    return embeddings

import faiss
import numpy as np

def create_faiss_index(embeddings):
    """Stores embeddings in a FAISS index."""
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(np.array(embeddings))
    return index

def query_embedding(query, model):
    """Generates embedding for the user query."""
    return model.encode([query])[0]

def retrieve_chunks(query_embedding, index, chunks, k=5):
    """Retrieves the top-k relevant chunks."""
    distances, indices = index.search(np.array([query_embedding]), k)
    return [chunks[i] for i in indices[0]]

def extract_comparison_data(relevant_chunks, field_name):
    """Extracts and compares data for a specific field."""
    comparison_data = []
    for chunk in relevant_chunks:
        if field_name in chunk:
            comparison_data.append(chunk)
    return comparison_data

import openai

def generate_response(context, query):
    """Generates a response using LLM."""
    prompt = f"Based on the following context, answer the query:\n\nContext:\n{context}\n\nQuery:\n{query}"
    response = openai.Completion.create(
        engine="text-davinci-003",
        prompt=prompt,
        max_tokens=500
    )
    return response.choices[0].text.strip()

from flask import Flask, request, render_template, jsonify

app = Flask(__name__)

# Global variables for simplicity
pdf_chunks = []
faiss_index = None
model = SentenceTransformer('all-MiniLM-L6-v2')

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/upload', methods=['POST'])
def upload_pdf():
    global pdf_chunks, faiss_index
    file = request.files['file']
    file.save('uploaded.pdf')

    # Process the PDF
    pdf_chunks = extract_and_chunk_pdf('uploaded.pdf')
    embeddings = create_embeddings(pdf_chunks)
    faiss_index = create_faiss_index(embeddings)
    return jsonify({"message": "PDF processed successfully."})

@app.route('/query', methods=['POST'])
def query_pdf():
    query = request.json.get('query')
    query_embed = query_embedding(query, model)
    relevant_chunks = retrieve_chunks(query_embed, faiss_index, pdf_chunks)

    # Generate a response
    context = " ".join(relevant_chunks)
    response = generate_response(context, query)
    return jsonify({"response": response})

!pip install flask gradio sentence-transformers faiss-cpu

!pip install pyngrok

!pip install pdfplumber

pip install django

!pip install flask gradio sentence-transformers faiss-cpu

!pip install flask pdfplumber sentence-transformers faiss-cpu langchain openai tiktoken
!pip install pyngro



!python -m venv myenv
!myenv\Scripts\activate

pip install weaviate-client

import pdfplumber
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import gradio as gr
import os
from transformers import pipeline

# Install required packages
!pip install flask pdfplumber sentence-transformers faiss-cpu transformers gradio

def extract_and_chunk_pdf(file_path, chunk_size=200):
    """Extracts and chunks text from a PDF."""
    with pdfplumber.open(file_path) as pdf:
        text = ''
        for page in pdf.pages:
            text += page.extract_text()

    # Chunking the text into smaller granularity
    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]
    return chunks

def create_embeddings(chunks):
    """Creates embeddings for text chunks."""
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode(chunks)
    return embeddings

def create_faiss_index(embeddings):
    """Stores embeddings in a FAISS index."""
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(np.array(embeddings))
    return index

def query_embedding(query, model):
    """Generates embedding for the user query."""
    return model.encode([query])[0]

def retrieve_chunks(query_embedding, index, chunks, k=5):
    """Retrieves the top-k relevant chunks."""
    distances, indices = index.search(np.array([query_embedding]), k)
    return [chunks[i] for i in indices[0]]

def answer_query(query):
    """Answers user query based on the PDF content."""
    global pdf_chunks, faiss_index, model

    query_embedding = model.encode([query])[0]
    relevant_chunks = retrieve_chunks(query_embedding, faiss_index, pdf_chunks)

    # Use Hugging Face pipeline for question answering
    qa_pipeline = pipeline("question-answering", model="distilbert-base-cased-distilled-squad")

    # Combine relevant chunks to form context
    context = " ".join(relevant_chunks)

    # Generate response using Hugging Face QA model
    response = qa_pipeline({
        "context": context,
        "question": query
    })

    return response["answer"]

# Global variables for simplicity
pdf_chunks = []
faiss_index = None
model = SentenceTransformer('all-MiniLM-L6-v2')

def process_pdf(file_obj):
    global pdf_chunks, faiss_index

    file_path = file_obj.name
    pdf_chunks = extract_and_chunk_pdf(file_path)
    embeddings = create_embeddings(pdf_chunks)
    faiss_index = create_faiss_index(embeddings)
    return "PDF processed successfully."

def query_pdf(query):
    if not pdf_chunks or not faiss_index:
        return "Please upload a PDF first"
    response = answer_query(query)
    return response

iface = gr.Interface(
    fn=process_pdf,
    inputs=gr.File(file_types=[".pdf"]),
    outputs="text",
    title="PDF Processing",
    description="Upload a PDF file to process it."
)

iface2 = gr.Interface(
    fn=query_pdf,
    inputs="text",
    outputs="text",
    title="Query PDF",
    description="Enter a query about the PDF."
)

demo = gr.TabbedInterface([iface, iface2], ["Upload PDF", "Query PDF"])
demo.launch(debug=True)

